{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1nuwSArvG9lr0q3f_ETOY7lM-A4ppsNnV","timestamp":1671205966250}],"machine_shape":"hm","toc_visible":true,"authorship_tag":"ABX9TyMqSIyVV895csCY4YPGD9d8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"69baefd594da4caf933aa3a5edf2663e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fb082fe96d7b4cbd8623664ff27b2c11","IPY_MODEL_550edbb4bee446edbbc14305391700eb","IPY_MODEL_3a0ef82034f2428fa5e786ff2ddbbaa9"],"layout":"IPY_MODEL_59c3b15dcfc34dd8a0c711a1e1e067f8"}},"fb082fe96d7b4cbd8623664ff27b2c11":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d781a7dfc6444394abadb62ffc15ea87","placeholder":"​","style":"IPY_MODEL_f50b3477fb95434cb5a9c930a4b00775","value":"Downloading: 100%"}},"550edbb4bee446edbbc14305391700eb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5c7df756806941b7bc748e14afea4ecc","max":160,"min":0,"orientation":"horizontal","style":"IPY_MODEL_eb34e49f8bcc453b93a0ada458dcba4d","value":160}},"3a0ef82034f2428fa5e786ff2ddbbaa9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bce385d42c6e4064a7646cf5f6c2ee1c","placeholder":"​","style":"IPY_MODEL_6ba200ae61f64a6297fe376b34b11491","value":" 160/160 [00:00&lt;00:00, 4.40kB/s]"}},"59c3b15dcfc34dd8a0c711a1e1e067f8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d781a7dfc6444394abadb62ffc15ea87":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f50b3477fb95434cb5a9c930a4b00775":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5c7df756806941b7bc748e14afea4ecc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb34e49f8bcc453b93a0ada458dcba4d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bce385d42c6e4064a7646cf5f6c2ee1c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6ba200ae61f64a6297fe376b34b11491":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fcbf4ae093764f5082bddf4083105e44":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7bb56f271f1a49d5825353e1f2f9fd34","IPY_MODEL_ffbfa863180d43d78ca008959366431f","IPY_MODEL_df7379d7f49b46fe98d8946b926b796e"],"layout":"IPY_MODEL_f7fb5468cc834ecaa4c3a669d97acd68"}},"7bb56f271f1a49d5825353e1f2f9fd34":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_72cb21823e16465bae66f2535ed4c0f9","placeholder":"​","style":"IPY_MODEL_1ac6fac2a5e74002b610d73728e50fc4","value":"Downloading: 100%"}},"ffbfa863180d43d78ca008959366431f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f142954fc62049f9b1716f021cb34190","max":502,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8c6e5d5b33f64a7694a79a30e30a6e19","value":502}},"df7379d7f49b46fe98d8946b926b796e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cb71907c28e3495f9ed3b7322e80893e","placeholder":"​","style":"IPY_MODEL_4aaffe0e039f45a3b5c3579b85b3d8cc","value":" 502/502 [00:00&lt;00:00, 12.3kB/s]"}},"f7fb5468cc834ecaa4c3a669d97acd68":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"72cb21823e16465bae66f2535ed4c0f9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1ac6fac2a5e74002b610d73728e50fc4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f142954fc62049f9b1716f021cb34190":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8c6e5d5b33f64a7694a79a30e30a6e19":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cb71907c28e3495f9ed3b7322e80893e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4aaffe0e039f45a3b5c3579b85b3d8cc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7fea85e4be434ba0a0e9c983f000ef02":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_87426e9fef12449f8536c70c877f5e12","IPY_MODEL_f03306cd6c0244dcbc032b9808f56d8a","IPY_MODEL_85451cc1279f4d229a35a6630f04dafc"],"layout":"IPY_MODEL_1ea8e8a825984323bce5917bf652ff6e"}},"87426e9fef12449f8536c70c877f5e12":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4900311c545044b8a3c683c95da595c0","placeholder":"​","style":"IPY_MODEL_68f7387ce6cd41279f26ee990d3f86f1","value":"Downloading: 100%"}},"f03306cd6c0244dcbc032b9808f56d8a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f934f68d1a1f45508187ae3ed8255cd9","max":345636463,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e979ae5f1d8b46b095f1540264f283ca","value":345636463}},"85451cc1279f4d229a35a6630f04dafc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7a63ca84af7f4b658aefe078d9784997","placeholder":"​","style":"IPY_MODEL_adfff6418dfc4e48997fe720d8643976","value":" 346M/346M [00:05&lt;00:00, 66.2MB/s]"}},"1ea8e8a825984323bce5917bf652ff6e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4900311c545044b8a3c683c95da595c0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"68f7387ce6cd41279f26ee990d3f86f1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f934f68d1a1f45508187ae3ed8255cd9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e979ae5f1d8b46b095f1540264f283ca":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7a63ca84af7f4b658aefe078d9784997":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"adfff6418dfc4e48997fe720d8643976":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# Tanpopo 表面付着物 ViT CrossValidation"],"metadata":{"id":"yxs2Ne0fc38A"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zh9ifXxPmVV3","executionInfo":{"status":"ok","timestamp":1671207423906,"user_tz":-540,"elapsed":13701,"user":{"displayName":"Zen Nakamura","userId":"00131218698156176369"}},"outputId":"c2289e44-4efa-48a2-867c-b240404efcd8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n","\u001b[K     |████████████████████████████████| 5.8 MB 6.6 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n","Collecting huggingface-hub<1.0,>=0.10.0\n","  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n","\u001b[K     |████████████████████████████████| 182 kB 65.8 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[K     |████████████████████████████████| 7.6 MB 68.3 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n","transformers                  4.25.1\n"]}],"source":["!pip install transformers\n","!pip list | grep transformers"]},{"cell_type":"code","source":["# Google Colab マウント\n","from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive\n","import os\n","os.chdir('/content/drive/MyDrive/Tanpopo')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OiX-Axl0sqBm","executionInfo":{"status":"ok","timestamp":1671207442102,"user_tz":-540,"elapsed":18213,"user":{"displayName":"Zen Nakamura","userId":"00131218698156176369"}},"outputId":"411adda6-8cd2-40e6-87e6-4426f51a3a73"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive\n"]}]},{"cell_type":"code","source":["from PIL import Image\n","import requests\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","from torchvision import datasets, models, transforms\n","import torch.utils.data as data\n","\n","import glob\n","import time\n","import copy\n","from PIL import Image"],"metadata":{"id":"grH6p9ChzOet"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#画像サイズがが704x480 #88x60\n","img_size = 224\n","\n","#class_names = ['1Sputter', '2Fiber', '3Block', '4Bar', '5AGFragment']\n","class_num = 5\n","\n","# 標準化\n","mean = (0.5, 0.5, 0.5)\n","std = (0.5, 0.5, 0.5)\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","#torch.set_default_tensor_type('torch.cuda.FloatTensor')"],"metadata":{"id":"jL9O2quezaEE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size =  \"16\" #@param[8, 16, 32, 64, 128, 256]\n","batch_size = int(batch_size)\n","\n","epochs = \"25\" #@param[5, 8, 10, 15, 20, 22, 25, 27, 29, 30, 31, 32, 33, 35, 45, 60, 120]\n","epochs = int(epochs)"],"metadata":{"id":"emBXp2djzasI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 関数、クラスの定義"],"metadata":{"id":"Aljd2m4Ht6w7"}},{"cell_type":"code","source":["import random\n","from sklearn.model_selection import train_test_split\n","def make_filepath_list(folderpath, phase='train'):\n","    \"\"\"\n","    ファイルのパスを格納したリストを返す\n","    \"\"\"\n","    # .DS_Storeが最初に読み込まれる\n","    file_list = []\n","    files_list = []\n","    class_names = []\n","\n","    for index, top_dir in enumerate(sorted(os.listdir(folderpath))):\n","        file_dir = os.path.join(folderpath, top_dir)\n","        file_list = glob.glob(file_dir + '/*bmp')\n","\n","        if top_dir != '.DS_Store':\n","            class_names.append(top_dir)\n","            files_list += [os.path.join(folderpath, top_dir, file).replace('\\\\', '/') for file in file_list]\n","                                                            \n","    return files_list, class_names"],"metadata":{"id":"3cpotOZvvPed"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train_list1 = os.listdir('/content/drive/MyDrive/Tanpopo/TrainingData11')\n","# test_list1 = os.listdir('/content/drive/MyDrive/Tanpopo/TestData11')\n","# print('trian : ', train_list1)\n","# print('trian : ', sorted(train_list1))\n","# print('test : ', test_list1)\n","# print('test : ', sorted(test_list1))"],"metadata":{"id":"UbS2OnXvUyem"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ImageTransform(object):\n","    \"\"\"\n","    画像の前処理\n","    \"\"\"\n","    def __init__(self, resize, mean, std):\n","        self.data_transform = {\n","            'train': transforms.Compose([\n","                # データオグメンテーション\n","                transforms.Resize(256), #リサイズ\n","                transforms.CenterCrop(resize), #切り取り\n","                transforms.RandomRotation(45), #ランタムに回転\n","                transforms.ColorJitter(), #ランダムに明るさ、コントラスト、彩度、色相を変化\n","                transforms.RandomHorizontalFlip(), #ランダムに左右(水平)反転\n","                transforms.RandomVerticalFlip(), #ランダムに上下(垂直)反転\n","                transforms.ToTensor(),\n","                transforms.Normalize(mean, std), #zcaと交換？\n","                #ZCA whitening追加する\n","            ]),\n","            'valid': transforms.Compose([\n","                transforms.Resize(256),\n","                transforms.CenterCrop(resize),\n","                transforms.ToTensor(),\n","                transforms.Normalize(mean, std),\n","            ]),\n","            'test': transforms.Compose([\n","                transforms.Resize(256),\n","                transforms.CenterCrop(resize),\n","                transforms.ToTensor(),\n","                transforms.Normalize(mean, std),\n","            ])\n","        }\n","\n","    def __call__(self, img, phase='train'):\n","        return self.data_transform[phase](img)"],"metadata":{"id":"HN6P97HFyun6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torchvision.io import read_image\n","\n","class SurfaceObjectDataset(data.Dataset):\n","    \"\"\"\n","    表面付着物のDatasetクラス\n","    PyTorchのDatasetクラスを継承\n","    \"\"\"\n","    def __init__(self, file_list, classes, transform=None, phase='train'):\n","        #super().__init__()\n","        self.file_list = file_list\n","        self.transform = transform\n","        self.classes = classes\n","        self.phase = phase\n","\n","        #self.img = None\n","        #self.label = None\n","\n","    def __len__(self):\n","        \"\"\"\n","        画像の枚数を返す\n","        \"\"\"\n","        return len(self.file_list)\n","\n","    def __getitem__(self, index):\n","        \"\"\"\n","        前処理した画像データのTensor形式のデータとラベルを取得\n","        \"\"\"\n","        # 指定したindexの画像を読み込む\n","        img_path = self.file_list[index]\n","        img = Image.open(img_path)\n","        #img = read_image(img_path)\n","\n","        # 画像ラベルをファイル名から抜き出す\n","        label = self.file_list[index].split('/')[6][:11]\n","\n","        # ラベル名を数値に変換\n","        label = self.classes.index(label)\n","        #print(label, end=' ')\n","\n","        # 画像の前処理を実施\n","        if self.transform is not None:\n","            img_transformed = self.transform(img, self.phase)\n","        \n","        #self.img = img_transformed\n","        #elf.label = label\n","\n","        return img_transformed, label"],"metadata":{"id":"ZqSGwQq6yxKG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_model(model, dataloaders, criterion, optimizer, num_epochs=25):\n","    since = time.time()\n","\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_acc = 0.0\n","\n","    for epoch in range(num_epochs):\n","        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n","\n","        # 各エポックには訓練フェーズと検証フェーズがあります\n","        for phase in ['train', 'valid']:\n","            if phase == 'train':\n","                model.train()  # モデルを訓練モードに設定します\n","            else:\n","                model.eval()   # モードを評価するモデルを設定します\n","\n","            running_loss = 0.0\n","            running_corrects = 0\n","\n","            # データをイレテートします\n","            ##i = 0\n","            for inputs, labels in dataloaders[phase]:\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                # パラメータの勾配をゼロにします\n","                optimizer.zero_grad()\n","\n","                # 順伝播\n","                # 訓練の時だけ、履歴を保持します\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    outputs = model(inputs)\n","                    _, preds = torch.max(outputs, 1)\n","                    loss = criterion(outputs, labels)\n","\n","                    # 訓練の時だけ逆伝播＋オプティマイズを行います\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                # 損失を計算します\n","                running_loss += loss.item() * inputs.size(0)\n","                running_corrects += torch.sum(preds == labels.data)\n","\n","                # # TensorBoardで可視化\n","                # if i % 8 == 7: # every 8 mini-batches\n","                #     if phase == 'train':\n","                #         writer.add_scalar('training loss', running_loss / 8,\n","                #                           epoch * len(dataloaders_dict[phase]) + i)\n","                #         writer.add_figure('Train predictions vs. actuals',\n","                #             plot_classes_preds(model, inputs, labels),\n","                #             global_step=epoch * len(dataloaders_dict[phase]) + i)\n","                #     elif phase == 'valid':\n","                #         writer.add_scalar('training loss', running_loss / 8,\n","                #                           epoch * len(dataloaders_dict[phase]) + i)\n","                #         writer.add_figure('Validation predictions vs. actuals',\n","                #             plot_classes_preds(model, inputs, labels),\n","                #             global_step=epoch * len(dataloaders_dict[phase]) + i)\n","                        \n","                # i += 1\n","                        \n","\n","            # if phase == 'train':\n","            #     scheduler.step()\n","\n","            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n","            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n","\n","            print(' {} Loss: {:.4f} Acc: {:.4f} '.format(\n","                phase, epoch_loss, epoch_acc), end='\\t')\n","\n","            # モデルをディープ・コピーします\n","            if phase == 'valid' and epoch_acc > best_acc:\n","                best_acc = epoch_acc\n","                best_model_wts = copy.deepcopy(model.state_dict())\n","\n","        print()\n","        #print('-' * 10)\n","\n","    time_elapsed = time.time() - since\n","    print('Training complete in {:.0f}m {:.0f}s'.format(\n","        time_elapsed // 60, time_elapsed % 60))\n","    print('Best val Acc: {:4f}'.format(best_acc))\n","\n","    # ベストモデルの重みをロードします\n","    model.load_state_dict(best_model_wts)\n","    return model, best_acc"],"metadata":{"id":"BkIJzkGZGkfF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix\n","import numpy as np\n","\n","#正解\n","def class_accuracy(label, conf_mat):\n","    return (conf_mat[label][label] + (np.sum(conf_mat) - (np.sum(conf_mat[:, label])+np.sum(conf_mat[label])-conf_mat[label][label]))) / np.sum(conf_mat)\n","    \n","#精度(適合率)\n","def class_precision(label, conf_mat):\n","    return conf_mat[label][label] / np.sum(conf_mat[label])\n","\n","#再現率\n","def class_recall(label, conf_mat):\n","    return conf_mat[label][label] / np.sum(conf_mat[:, label])\n","\n","# テスト結果を返す\n","def test_model(model, test_dataloaders):\n","    labels_sum = None\n","    predicted_sum = None\n","\n","    with torch.no_grad():\n","        for data in test_dataloaders:\n","            images, labels = data\n","            images = images.to(device)\n","            labels = labels.to(device)\n","\n","            outputs = model(images)\n","\n","            #outputs = nn.Softmax(dim=1)(outputs)\n","\n","            _, predicted = torch.max(outputs, 1)\n","\n","            if labels_sum is None:\n","                labels_sum = labels\n","                predicted_sum = predicted\n","            else:\n","                labels_sum = torch.cat([labels_sum, labels], dim=0)\n","                predicted_sum = torch.cat([predicted_sum, predicted], dim=0)\n","\n","    #混同行列\n","    labels_sum = labels_sum.cpu()\n","    predicted_sum = predicted_sum.cpu()\n","    conf_mat = None\n","    Accuracy = []\n","    Precision = []\n","    Recall = []\n","\n","    conf_mat = confusion_matrix(labels_sum, predicted_sum)\n","\n","    for i in range(class_num):\n","        Accuracy = np.append(Accuracy, class_accuracy(i, conf_mat)*100)\n","        Precision = np.append(Precision, class_precision(i, conf_mat)*100)\n","        Recall = np.append(Recall, class_recall(i, conf_mat)*100)\n","\n","    return conf_mat, Accuracy, Precision, Recall"],"metadata":{"id":"_awmlxSzsi0s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### モデルの作成"],"metadata":{"id":"vQ7pl5oht14e"}},{"cell_type":"code","source":["from transformers import ViTFeatureExtractor, ViTModel\n","# ファインチューニングされたモデルをロードして使う場合はViTForImageClassificationですぐに分類問題に適用できるようですが、\n","# 今回はファインチューニングの実装のところからも行いたいので、こちらは使いません。\n","# from transformers import ViTForImageClassification\n","\n","# 前処理用クラス\n","feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n","\n","# モデル本体\n","# 順伝播時の出力にAttentionの結果もほしいときはoutput_attentions=Trueを指定する。\n","vit_model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k', output_attentions=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["69baefd594da4caf933aa3a5edf2663e","fb082fe96d7b4cbd8623664ff27b2c11","550edbb4bee446edbbc14305391700eb","3a0ef82034f2428fa5e786ff2ddbbaa9","59c3b15dcfc34dd8a0c711a1e1e067f8","d781a7dfc6444394abadb62ffc15ea87","f50b3477fb95434cb5a9c930a4b00775","5c7df756806941b7bc748e14afea4ecc","eb34e49f8bcc453b93a0ada458dcba4d","bce385d42c6e4064a7646cf5f6c2ee1c","6ba200ae61f64a6297fe376b34b11491","fcbf4ae093764f5082bddf4083105e44","7bb56f271f1a49d5825353e1f2f9fd34","ffbfa863180d43d78ca008959366431f","df7379d7f49b46fe98d8946b926b796e","f7fb5468cc834ecaa4c3a669d97acd68","72cb21823e16465bae66f2535ed4c0f9","1ac6fac2a5e74002b610d73728e50fc4","f142954fc62049f9b1716f021cb34190","8c6e5d5b33f64a7694a79a30e30a6e19","cb71907c28e3495f9ed3b7322e80893e","4aaffe0e039f45a3b5c3579b85b3d8cc","7fea85e4be434ba0a0e9c983f000ef02","87426e9fef12449f8536c70c877f5e12","f03306cd6c0244dcbc032b9808f56d8a","85451cc1279f4d229a35a6630f04dafc","1ea8e8a825984323bce5917bf652ff6e","4900311c545044b8a3c683c95da595c0","68f7387ce6cd41279f26ee990d3f86f1","f934f68d1a1f45508187ae3ed8255cd9","e979ae5f1d8b46b095f1540264f283ca","7a63ca84af7f4b658aefe078d9784997","adfff6418dfc4e48997fe720d8643976"]},"id":"0H-GjJiNpH6p","executionInfo":{"status":"ok","timestamp":1671207456378,"user_tz":-540,"elapsed":10789,"user":{"displayName":"Zen Nakamura","userId":"00131218698156176369"}},"outputId":"6220dfb0-4507-4874-b505-c980b10dc438"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/160 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69baefd594da4caf933aa3a5edf2663e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/502 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fcbf4ae093764f5082bddf4083105e44"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/346M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7fea85e4be434ba0a0e9c983f000ef02"}},"metadata":{}}]},{"cell_type":"code","source":["print(vit_model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X63VMgy9tLuW","executionInfo":{"status":"ok","timestamp":1671207456378,"user_tz":-540,"elapsed":6,"user":{"displayName":"Zen Nakamura","userId":"00131218698156176369"}},"outputId":"bec2fbe1-ac56-4174-faf4-17df1908238a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ViTModel(\n","  (embeddings): ViTEmbeddings(\n","    (patch_embeddings): ViTPatchEmbeddings(\n","      (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n","    )\n","    (dropout): Dropout(p=0.0, inplace=False)\n","  )\n","  (encoder): ViTEncoder(\n","    (layer): ModuleList(\n","      (0): ViTLayer(\n","        (attention): ViTAttention(\n","          (attention): ViTSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","          (output): ViTSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","        )\n","        (intermediate): ViTIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): ViTOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.0, inplace=False)\n","        )\n","        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (1): ViTLayer(\n","        (attention): ViTAttention(\n","          (attention): ViTSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","          (output): ViTSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","        )\n","        (intermediate): ViTIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): ViTOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.0, inplace=False)\n","        )\n","        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (2): ViTLayer(\n","        (attention): ViTAttention(\n","          (attention): ViTSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","          (output): ViTSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","        )\n","        (intermediate): ViTIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): ViTOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.0, inplace=False)\n","        )\n","        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (3): ViTLayer(\n","        (attention): ViTAttention(\n","          (attention): ViTSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","          (output): ViTSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","        )\n","        (intermediate): ViTIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): ViTOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.0, inplace=False)\n","        )\n","        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (4): ViTLayer(\n","        (attention): ViTAttention(\n","          (attention): ViTSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","          (output): ViTSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","        )\n","        (intermediate): ViTIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): ViTOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.0, inplace=False)\n","        )\n","        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (5): ViTLayer(\n","        (attention): ViTAttention(\n","          (attention): ViTSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","          (output): ViTSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","        )\n","        (intermediate): ViTIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): ViTOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.0, inplace=False)\n","        )\n","        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (6): ViTLayer(\n","        (attention): ViTAttention(\n","          (attention): ViTSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","          (output): ViTSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","        )\n","        (intermediate): ViTIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): ViTOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.0, inplace=False)\n","        )\n","        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (7): ViTLayer(\n","        (attention): ViTAttention(\n","          (attention): ViTSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","          (output): ViTSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","        )\n","        (intermediate): ViTIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): ViTOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.0, inplace=False)\n","        )\n","        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (8): ViTLayer(\n","        (attention): ViTAttention(\n","          (attention): ViTSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","          (output): ViTSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","        )\n","        (intermediate): ViTIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): ViTOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.0, inplace=False)\n","        )\n","        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (9): ViTLayer(\n","        (attention): ViTAttention(\n","          (attention): ViTSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","          (output): ViTSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","        )\n","        (intermediate): ViTIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): ViTOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.0, inplace=False)\n","        )\n","        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (10): ViTLayer(\n","        (attention): ViTAttention(\n","          (attention): ViTSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","          (output): ViTSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","        )\n","        (intermediate): ViTIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): ViTOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.0, inplace=False)\n","        )\n","        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (11): ViTLayer(\n","        (attention): ViTAttention(\n","          (attention): ViTSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","          (output): ViTSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","        )\n","        (intermediate): ViTIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): ViTOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.0, inplace=False)\n","        )\n","        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","    )\n","  )\n","  (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","  (pooler): ViTPooler(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (activation): Tanh()\n","  )\n",")\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","class ViTNet(nn.Module):\n","    def __init__(self, pretrained_vit_model, class_num):\n","        super(ViTNet, self).__init__()\n","        self.vit = pretrained_vit_model\n","        self.fc = nn.Linear(768, class_num)\n","\n","    def _get_cls_vec(self, states): # fine tuningで\n","        return states['last_hidden_state'][:, 0, :]\n","\n","    def forward(self, input_ids):\n","        states = self.vit(input_ids)\n","        states = self._get_cls_vec(states)\n","        states = self.fc(states)\n","        return states\n","    \n","model_ViT = ViTNet(vit_model, class_num)\n","#model_ViT.to(device)"],"metadata":{"id":"FROwve_I_6WM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# まず全パラメータを勾配計算Falseにする\n","for param in model_ViT.parameters():\n","    param.requires_grad = False\n","\n","# 追加したクラス分類用の全結合層を勾配計算ありに変更\n","for param in model_ViT.fc.parameters():\n","    param.requires_grad = True\n","\n","optimizer = optim.AdamW([\n","    {'params': model_ViT.fc.parameters(), 'lr': 0.001, 'weight_decay': 0.5}\n","    #optim.AdamW(batchNorm_params_to_update, lr=0.001, weight_decay=0.001)\n","])\n","# optimizer = optim.RAdam([\n","#     {'params': model_ViT.fc.parameters(), 'lr': 0.001, 'weight_decay': 0.1}\n","# ])\n","\n","criterion = nn.CrossEntropyLoss()"],"metadata":{"id":"7N0iP_UnEKWb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 学習"],"metadata":{"id":"CzFcxzlTkiKn"}},{"cell_type":"code","source":["from torch.utils.data.dataset import Subset\n","from sklearn.model_selection import KFold\n","from  torch.utils.data import Dataset\n","\n","# 訓練、検証データへのファイルパスを格納したリストを取得\n","train_file_list, class_names = make_filepath_list('/content/drive/MyDrive/Tanpopo/TrainingData11', 'train')\n","#print('train_file_list: ', train_file_list)\n","#print('class_names: ', class_names)\n","class_num = len(class_names) # 5\n","\n","dataset = SurfaceObjectDataset(\n","        file_list = train_file_list, classes = class_names,\n","        transform = ImageTransform(img_size, mean, std),\n","        phase = 'train')\n","\n","# テストデータ\n","test_file_list, class_names_test = make_filepath_list('/content/drive/MyDrive/Tanpopo/TestData11', 'test')\n","#print('test_file_list : ', test_file_list)\n","#print('class_names_test : ', class_names_test)\n","\n","# Datasetの作成\n","test_dataset = SurfaceObjectDataset(\n","    file_list = test_file_list, classes = class_names_test,\n","    transform = ImageTransform(img_size, mean, std),\n","    phase = 'test')\n","# Dataloaderの作成\n","test_dataloader = data.DataLoader(\n","    test_dataset, batch_size = int(batch_size/2), shuffle=False)\n","dataloaders_dict = {'test': test_dataloader}\n","\n","\n","# K-Fold 交差検証 \n","kf = KFold(n_splits=5, shuffle=True, random_state=1) # 5回検証\n","\n","scores = []\n","test_accuracy = []\n","test_precision = []\n","test_recall = []\n","conf_mat = []\n","\n","PATH = '/content/drive/MyDrive/Tanpopo/model_ViT_weights.pth'\n","model_ViT = model_ViT.to(device)\n","model_ViT.load_state_dict(torch.load(PATH)) # 学習前: 学習済みの重みを使う\n","\n","for _fold, (train_index, valid_index) in enumerate(kf.split(train_file_list)):\n","    # Datasetの作成\n","    train_dataset = Subset(dataset, train_index)\n","    valid_dataset = Subset(dataset, valid_index)\n","    # Dataloaderの作成\n","    train_dataloader = data.DataLoader(\n","        train_dataset, batch_size = batch_size, shuffle=True)\n","    valid_dataloader = data.DataLoader(\n","        valid_dataset, batch_size = int(batch_size/2), shuffle=False)\n","    \n","    dataloaders_dict['train'] = train_dataloader\n","    dataloaders_dict['valid'] = valid_dataloader\n","\n","    print('-'*5, end='')\n","    print(_fold+1, end='')\n","    print('-'*5)\n","    model_ViT = model_ViT.to(device)\n","    model_ViT, score = train_model(model_ViT, dataloaders_dict, criterion, optimizer, num_epochs=epochs)\n","    scores.append(score.to('cpu'))\n","\n","    conf, test_acc, test_prec, test_rec = test_model(model_ViT, dataloaders_dict['test'])\n","\n","    test_accuracy.append(test_acc)\n","    test_precision.append(test_prec)\n","    test_recall.append(test_rec)\n","    conf_mat.append(conf)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FQ57KVgKy0MU","executionInfo":{"status":"ok","timestamp":1671208254227,"user_tz":-540,"elapsed":660686,"user":{"displayName":"Zen Nakamura","userId":"00131218698156176369"}},"outputId":"018409cf-39c0-4fe4-d087-7c3a9e68b5e2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["-----1-----\n","Epoch 0/24\n"," train Loss: 0.1634 Acc: 0.9643 \t valid Loss: 0.1205 Acc: 0.9796 \t\n","Epoch 1/24\n"," train Loss: 0.1608 Acc: 0.9490 \t valid Loss: 0.1411 Acc: 1.0000 \t\n","Epoch 2/24\n"," train Loss: 0.1637 Acc: 0.9796 \t valid Loss: 0.1250 Acc: 0.9796 \t\n","Epoch 3/24\n"," train Loss: 0.1659 Acc: 0.9643 \t valid Loss: 0.1427 Acc: 0.9796 \t\n","Epoch 4/24\n"," train Loss: 0.1560 Acc: 0.9745 \t valid Loss: 0.1282 Acc: 0.9796 \t\n","Epoch 5/24\n"," train Loss: 0.1737 Acc: 0.9643 \t valid Loss: 0.0973 Acc: 0.9796 \t\n","Epoch 6/24\n"," train Loss: 0.1554 Acc: 0.9592 \t valid Loss: 0.1481 Acc: 0.9796 \t\n","Epoch 7/24\n"," train Loss: 0.1427 Acc: 0.9745 \t valid Loss: 0.1250 Acc: 0.9796 \t\n","Epoch 8/24\n"," train Loss: 0.1493 Acc: 0.9643 \t valid Loss: 0.1218 Acc: 1.0000 \t\n","Epoch 9/24\n"," train Loss: 0.1553 Acc: 0.9796 \t valid Loss: 0.2036 Acc: 0.9388 \t\n","Epoch 10/24\n"," train Loss: 0.1451 Acc: 0.9694 \t valid Loss: 0.1232 Acc: 0.9796 \t\n","Epoch 11/24\n"," train Loss: 0.1640 Acc: 0.9592 \t valid Loss: 0.0984 Acc: 1.0000 \t\n","Epoch 12/24\n"," train Loss: 0.1163 Acc: 0.9898 \t valid Loss: 0.1243 Acc: 1.0000 \t\n","Epoch 13/24\n"," train Loss: 0.1363 Acc: 0.9745 \t valid Loss: 0.1085 Acc: 1.0000 \t\n","Epoch 14/24\n"," train Loss: 0.1505 Acc: 0.9745 \t valid Loss: 0.1625 Acc: 0.9796 \t\n","Epoch 15/24\n"," train Loss: 0.1388 Acc: 0.9898 \t valid Loss: 0.1078 Acc: 1.0000 \t\n","Epoch 16/24\n"," train Loss: 0.1461 Acc: 0.9745 \t valid Loss: 0.1725 Acc: 0.9796 \t\n","Epoch 17/24\n"," train Loss: 0.1364 Acc: 0.9643 \t valid Loss: 0.1467 Acc: 0.9796 \t\n","Epoch 18/24\n"," train Loss: 0.1640 Acc: 0.9643 \t valid Loss: 0.1300 Acc: 1.0000 \t\n","Epoch 19/24\n"," train Loss: 0.1391 Acc: 0.9745 \t valid Loss: 0.1494 Acc: 0.9796 \t\n","Epoch 20/24\n"," train Loss: 0.1847 Acc: 0.9592 \t valid Loss: 0.1546 Acc: 0.9796 \t\n","Epoch 21/24\n"," train Loss: 0.1460 Acc: 0.9745 \t valid Loss: 0.1392 Acc: 0.9796 \t\n","Epoch 22/24\n"," train Loss: 0.1395 Acc: 0.9592 \t valid Loss: 0.1532 Acc: 0.9796 \t\n","Epoch 23/24\n"," train Loss: 0.1483 Acc: 0.9592 \t valid Loss: 0.1175 Acc: 0.9796 \t\n","Epoch 24/24\n"," train Loss: 0.1642 Acc: 0.9643 \t valid Loss: 0.1669 Acc: 0.9796 \t\n","Training complete in 3m 22s\n","Best val Acc: 1.000000\n","-----2-----\n","Epoch 0/24\n"," train Loss: 0.1361 Acc: 0.9898 \t valid Loss: 0.1559 Acc: 0.9796 \t\n","Epoch 1/24\n"," train Loss: 0.1378 Acc: 0.9643 \t valid Loss: 0.1726 Acc: 0.9592 \t\n","Epoch 2/24\n"," train Loss: 0.1594 Acc: 0.9694 \t valid Loss: 0.1406 Acc: 0.9796 \t\n","Epoch 3/24\n"," train Loss: 0.1262 Acc: 0.9949 \t valid Loss: 0.1544 Acc: 0.9796 \t\n","Epoch 4/24\n"," train Loss: 0.1600 Acc: 0.9643 \t valid Loss: 0.1441 Acc: 0.9796 \t\n","Epoch 5/24\n"," train Loss: 0.1682 Acc: 0.9643 \t valid Loss: 0.1971 Acc: 0.9796 \t\n","Epoch 6/24\n"," train Loss: 0.1438 Acc: 0.9796 \t valid Loss: 0.1663 Acc: 0.9796 \t\n","Epoch 7/24\n"," train Loss: 0.1606 Acc: 0.9898 \t valid Loss: 0.1938 Acc: 0.9592 \t\n","Epoch 8/24\n"," train Loss: 0.1318 Acc: 0.9745 \t valid Loss: 0.1547 Acc: 0.9796 \t\n","Epoch 9/24\n"," train Loss: 0.1436 Acc: 0.9898 \t valid Loss: 0.1902 Acc: 0.9592 \t\n","Epoch 10/24\n"," train Loss: 0.1499 Acc: 0.9745 \t valid Loss: 0.1421 Acc: 1.0000 \t\n","Epoch 11/24\n"," train Loss: 0.1535 Acc: 0.9745 \t valid Loss: 0.1501 Acc: 0.9796 \t\n","Epoch 12/24\n"," train Loss: 0.1307 Acc: 0.9847 \t valid Loss: 0.1766 Acc: 0.9796 \t\n","Epoch 13/24\n"," train Loss: 0.1275 Acc: 0.9847 \t valid Loss: 0.1622 Acc: 0.9796 \t\n","Epoch 14/24\n"," train Loss: 0.1638 Acc: 0.9592 \t valid Loss: 0.1872 Acc: 0.9796 \t\n","Epoch 15/24\n"," train Loss: 0.1556 Acc: 0.9541 \t valid Loss: 0.1370 Acc: 0.9796 \t\n","Epoch 16/24\n"," train Loss: 0.1556 Acc: 0.9796 \t valid Loss: 0.1573 Acc: 0.9796 \t\n","Epoch 17/24\n"," train Loss: 0.1449 Acc: 0.9745 \t valid Loss: 0.1787 Acc: 0.9796 \t\n","Epoch 18/24\n"," train Loss: 0.1501 Acc: 0.9796 \t valid Loss: 0.2043 Acc: 0.9592 \t\n","Epoch 19/24\n"," train Loss: 0.1454 Acc: 0.9898 \t valid Loss: 0.1779 Acc: 0.9592 \t\n","Epoch 20/24\n"," train Loss: 0.1261 Acc: 0.9847 \t valid Loss: 0.2063 Acc: 0.9388 \t\n","Epoch 21/24\n"," train Loss: 0.1384 Acc: 0.9643 \t valid Loss: 0.1794 Acc: 0.9796 \t\n","Epoch 22/24\n"," train Loss: 0.1280 Acc: 0.9796 \t valid Loss: 0.1926 Acc: 0.9796 \t\n","Epoch 23/24\n"," train Loss: 0.1427 Acc: 0.9796 \t valid Loss: 0.2198 Acc: 0.9592 \t\n","Epoch 24/24\n"," train Loss: 0.1537 Acc: 0.9643 \t valid Loss: 0.2310 Acc: 0.9388 \t\n","Training complete in 1m 43s\n","Best val Acc: 1.000000\n","-----3-----\n","Epoch 0/24\n"," train Loss: 0.1444 Acc: 0.9745 \t valid Loss: 0.2074 Acc: 0.9388 \t\n","Epoch 1/24\n"," train Loss: 0.1349 Acc: 0.9847 \t valid Loss: 0.2218 Acc: 0.9388 \t\n","Epoch 2/24\n"," train Loss: 0.1255 Acc: 0.9898 \t valid Loss: 0.2185 Acc: 0.9388 \t\n","Epoch 3/24\n"," train Loss: 0.1321 Acc: 0.9898 \t valid Loss: 0.2014 Acc: 0.9796 \t\n","Epoch 4/24\n"," train Loss: 0.1306 Acc: 0.9898 \t valid Loss: 0.1969 Acc: 0.9388 \t\n","Epoch 5/24\n"," train Loss: 0.1255 Acc: 0.9898 \t valid Loss: 0.2387 Acc: 0.9388 \t\n","Epoch 6/24\n"," train Loss: 0.1239 Acc: 0.9796 \t valid Loss: 0.2927 Acc: 0.9184 \t\n","Epoch 7/24\n"," train Loss: 0.1139 Acc: 0.9898 \t valid Loss: 0.2610 Acc: 0.8980 \t\n","Epoch 8/24\n"," train Loss: 0.1298 Acc: 0.9745 \t valid Loss: 0.2349 Acc: 0.9184 \t\n","Epoch 9/24\n"," train Loss: 0.1189 Acc: 0.9847 \t valid Loss: 0.2490 Acc: 0.9184 \t\n","Epoch 10/24\n"," train Loss: 0.1125 Acc: 1.0000 \t valid Loss: 0.2623 Acc: 0.9184 \t\n","Epoch 11/24\n"," train Loss: 0.1332 Acc: 0.9643 \t valid Loss: 0.2672 Acc: 0.9388 \t\n","Epoch 12/24\n"," train Loss: 0.1163 Acc: 0.9796 \t valid Loss: 0.2726 Acc: 0.8776 \t\n","Epoch 13/24\n"," train Loss: 0.1317 Acc: 0.9643 \t valid Loss: 0.2932 Acc: 0.8980 \t\n","Epoch 14/24\n"," train Loss: 0.1048 Acc: 0.9898 \t valid Loss: 0.2214 Acc: 0.9592 \t\n","Epoch 15/24\n"," train Loss: 0.1122 Acc: 0.9847 \t valid Loss: 0.2425 Acc: 0.9184 \t\n","Epoch 16/24\n"," train Loss: 0.1079 Acc: 0.9898 \t valid Loss: 0.2247 Acc: 0.9388 \t\n","Epoch 17/24\n"," train Loss: 0.1188 Acc: 0.9745 \t valid Loss: 0.2272 Acc: 0.9592 \t\n","Epoch 18/24\n"," train Loss: 0.1112 Acc: 0.9847 \t valid Loss: 0.2765 Acc: 0.9592 \t\n","Epoch 19/24\n"," train Loss: 0.1191 Acc: 0.9847 \t valid Loss: 0.2867 Acc: 0.8980 \t\n","Epoch 20/24\n"," train Loss: 0.1305 Acc: 0.9796 \t valid Loss: 0.2270 Acc: 0.9388 \t\n","Epoch 21/24\n"," train Loss: 0.1146 Acc: 0.9796 \t valid Loss: 0.3021 Acc: 0.8980 \t\n","Epoch 22/24\n"," train Loss: 0.1060 Acc: 0.9796 \t valid Loss: 0.2609 Acc: 0.9388 \t\n","Epoch 23/24\n"," train Loss: 0.1424 Acc: 0.9643 \t valid Loss: 0.2657 Acc: 0.9388 \t\n","Epoch 24/24\n"," train Loss: 0.1269 Acc: 0.9796 \t valid Loss: 0.2881 Acc: 0.9388 \t\n","Training complete in 1m 43s\n","Best val Acc: 0.979592\n","-----4-----\n","Epoch 0/24\n"," train Loss: 0.1741 Acc: 0.9643 \t valid Loss: 0.0675 Acc: 1.0000 \t\n","Epoch 1/24\n"," train Loss: 0.1472 Acc: 0.9796 \t valid Loss: 0.0781 Acc: 1.0000 \t\n","Epoch 2/24\n"," train Loss: 0.1514 Acc: 0.9694 \t valid Loss: 0.1052 Acc: 1.0000 \t\n","Epoch 3/24\n"," train Loss: 0.1677 Acc: 0.9796 \t valid Loss: 0.1133 Acc: 1.0000 \t\n","Epoch 4/24\n"," train Loss: 0.1574 Acc: 0.9643 \t valid Loss: 0.1521 Acc: 0.9592 \t\n","Epoch 5/24\n"," train Loss: 0.1527 Acc: 0.9745 \t valid Loss: 0.0909 Acc: 1.0000 \t\n","Epoch 6/24\n"," train Loss: 0.1489 Acc: 0.9745 \t valid Loss: 0.1322 Acc: 1.0000 \t\n","Epoch 7/24\n"," train Loss: 0.1824 Acc: 0.9592 \t valid Loss: 0.0945 Acc: 0.9796 \t\n","Epoch 8/24\n"," train Loss: 0.1713 Acc: 0.9745 \t valid Loss: 0.0848 Acc: 0.9796 \t\n","Epoch 9/24\n"," train Loss: 0.1560 Acc: 0.9847 \t valid Loss: 0.1203 Acc: 0.9796 \t\n","Epoch 10/24\n"," train Loss: 0.1668 Acc: 0.9694 \t valid Loss: 0.0924 Acc: 1.0000 \t\n","Epoch 11/24\n"," train Loss: 0.1478 Acc: 0.9796 \t valid Loss: 0.0928 Acc: 1.0000 \t\n","Epoch 12/24\n"," train Loss: 0.1515 Acc: 0.9694 \t valid Loss: 0.0798 Acc: 1.0000 \t\n","Epoch 13/24\n"," train Loss: 0.1550 Acc: 0.9592 \t valid Loss: 0.1288 Acc: 1.0000 \t\n","Epoch 14/24\n"," train Loss: 0.1334 Acc: 0.9796 \t valid Loss: 0.1233 Acc: 0.9592 \t\n","Epoch 15/24\n"," train Loss: 0.1401 Acc: 0.9796 \t valid Loss: 0.1010 Acc: 1.0000 \t\n","Epoch 16/24\n"," train Loss: 0.1670 Acc: 0.9643 \t valid Loss: 0.1638 Acc: 0.9592 \t\n","Epoch 17/24\n"," train Loss: 0.1398 Acc: 0.9796 \t valid Loss: 0.1128 Acc: 0.9796 \t\n","Epoch 18/24\n"," train Loss: 0.1451 Acc: 0.9694 \t valid Loss: 0.1269 Acc: 0.9796 \t\n","Epoch 19/24\n"," train Loss: 0.1377 Acc: 0.9796 \t valid Loss: 0.1046 Acc: 0.9796 \t\n","Epoch 20/24\n"," train Loss: 0.1394 Acc: 0.9745 \t valid Loss: 0.1083 Acc: 1.0000 \t\n","Epoch 21/24\n"," train Loss: 0.1523 Acc: 0.9694 \t valid Loss: 0.1272 Acc: 0.9796 \t\n","Epoch 22/24\n"," train Loss: 0.1468 Acc: 0.9694 \t valid Loss: 0.1293 Acc: 0.9796 \t\n","Epoch 23/24\n"," train Loss: 0.1262 Acc: 0.9694 \t valid Loss: 0.1794 Acc: 0.9592 \t\n","Epoch 24/24\n"," train Loss: 0.1672 Acc: 0.9541 \t valid Loss: 0.1306 Acc: 0.9796 \t\n","Training complete in 1m 44s\n","Best val Acc: 1.000000\n","-----5-----\n","Epoch 0/24\n"," train Loss: 0.1447 Acc: 0.9796 \t valid Loss: 0.0985 Acc: 1.0000 \t\n","Epoch 1/24\n"," train Loss: 0.1257 Acc: 0.9847 \t valid Loss: 0.1215 Acc: 0.9796 \t\n","Epoch 2/24\n"," train Loss: 0.1569 Acc: 0.9745 \t valid Loss: 0.0997 Acc: 0.9796 \t\n","Epoch 3/24\n"," train Loss: 0.1495 Acc: 0.9745 \t valid Loss: 0.1710 Acc: 0.9388 \t\n","Epoch 4/24\n"," train Loss: 0.1404 Acc: 0.9694 \t valid Loss: 0.1448 Acc: 0.9796 \t\n","Epoch 5/24\n"," train Loss: 0.1541 Acc: 0.9694 \t valid Loss: 0.1624 Acc: 0.9592 \t\n","Epoch 6/24\n"," train Loss: 0.1334 Acc: 0.9847 \t valid Loss: 0.1432 Acc: 0.9796 \t\n","Epoch 7/24\n"," train Loss: 0.1253 Acc: 0.9796 \t valid Loss: 0.1205 Acc: 0.9796 \t\n","Epoch 8/24\n"," train Loss: 0.1570 Acc: 0.9694 \t valid Loss: 0.1196 Acc: 0.9796 \t\n","Epoch 9/24\n"," train Loss: 0.1429 Acc: 0.9643 \t valid Loss: 0.1404 Acc: 0.9592 \t\n","Epoch 10/24\n"," train Loss: 0.1335 Acc: 0.9796 \t valid Loss: 0.1488 Acc: 0.9796 \t\n","Epoch 11/24\n"," train Loss: 0.1337 Acc: 0.9796 \t valid Loss: 0.1566 Acc: 0.9592 \t\n","Epoch 12/24\n"," train Loss: 0.1413 Acc: 0.9745 \t valid Loss: 0.1408 Acc: 0.9592 \t\n","Epoch 13/24\n"," train Loss: 0.1659 Acc: 0.9592 \t valid Loss: 0.1305 Acc: 0.9796 \t\n","Epoch 14/24\n"," train Loss: 0.1536 Acc: 0.9643 \t valid Loss: 0.1258 Acc: 0.9592 \t\n","Epoch 15/24\n"," train Loss: 0.1542 Acc: 0.9745 \t valid Loss: 0.1632 Acc: 0.9388 \t\n","Epoch 16/24\n"," train Loss: 0.1407 Acc: 0.9796 \t valid Loss: 0.1666 Acc: 0.9592 \t\n","Epoch 17/24\n"," train Loss: 0.1358 Acc: 0.9898 \t valid Loss: 0.1819 Acc: 0.8980 \t\n","Epoch 18/24\n"," train Loss: 0.1204 Acc: 0.9898 \t valid Loss: 0.1775 Acc: 0.9592 \t\n","Epoch 19/24\n"," train Loss: 0.1354 Acc: 0.9796 \t valid Loss: 0.1376 Acc: 0.9592 \t\n","Epoch 20/24\n"," train Loss: 0.1477 Acc: 0.9745 \t valid Loss: 0.1307 Acc: 0.9388 \t\n","Epoch 21/24\n"," train Loss: 0.1444 Acc: 0.9643 \t valid Loss: 0.1606 Acc: 0.9388 \t\n","Epoch 22/24\n"," train Loss: 0.1480 Acc: 0.9745 \t valid Loss: 0.1744 Acc: 0.9592 \t\n","Epoch 23/24\n"," train Loss: 0.1377 Acc: 0.9796 \t valid Loss: 0.1675 Acc: 0.9184 \t\n","Epoch 24/24\n"," train Loss: 0.1434 Acc: 0.9745 \t valid Loss: 0.1683 Acc: 0.9592 \t\n","Training complete in 1m 43s\n","Best val Acc: 1.000000\n"]}]},{"cell_type":"code","source":["# Validation \n","print('ViT Result')\n","print(f'Validation Accuracy: 平均 {np.mean(scores)*100:.1f} 標準偏差 {np.std(cores)*100:.4f}')\n","\n","# Test\n","print('Test results:')\n","print(class_names)\n","\n","tmp_list = [] # Accuracy\n","print('Accuracy:', end='\\t')\n","for i in range(class_num):\n","    tmp_list = [r[i] for r in test_accuracy]\n","    means = np.mean(tmp_list)\n","    stds = np.std(tmp_list)\n","    print(f'{np.mean(tmp_list):.1f}±{np.std(tmp_list):.2f}', end='\\t')\n","\n","tmp_list = [] # Precision\n","print('\\nPrecision:', end='\\t')\n","for i in range(class_num):\n","    tmp_list = [r[i] for r in test_precision]\n","    means = np.mean(tmp_list)\n","    stds = np.std(tmp_list)\n","    print(f'{np.mean(tmp_list):.1f}±{np.std(tmp_list):.2f}', end='\\t')\n","\n","tmp_list = [] # Recall\n","print('\\nRecall:', end='\\t')\n","for i in range(class_num):\n","    tmp_list = [r[i] for r in test_recall]\n","    means = np.mean(tmp_list)\n","    stds = np.std(tmp_list)\n","    print(f'{np.mean(tmp_list):.1f}±{np.std(tmp_list):.2f}', end='\\t')\n","\n","print('\\nConf Matrix:') # 混同行列\n","for i in range(class_num):\n","    print([r[i] for r in conf_mat])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Civa8-vUstzc","executionInfo":{"status":"ok","timestamp":1671208378454,"user_tz":-540,"elapsed":7,"user":{"displayName":"Zen Nakamura","userId":"00131218698156176369"}},"outputId":"90d9d319-41da-4bad-f0fc-f00473a76650"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ViT Result\n","Validation Accuracy: 平均 99.6 分散 0.0067\n","Test results:\n","['1Sputter', '2Fiber', '3Block', '4Bar', '5AGFragment']\n","Accuracy:\t99.8±0.38\t97.9±0.38\t97.1±0.00\t99.0±0.00\t96.2±0.00\t\n","Precision:\t99.0±1.90\t100.0±0.00\t90.5±0.00\t95.2±0.00\t90.5±0.00\t\n","Recall:\t100.0±0.00\t90.5±1.52\t95.0±0.00\t100.0±0.00\t90.5±0.00\t\n","Conf Matrix:\n","[array([21,  0,  0,  0,  0]), array([21,  0,  0,  0,  0]), array([20,  1,  0,  0,  0]), array([21,  0,  0,  0,  0]), array([21,  0,  0,  0,  0])]\n","[array([ 0, 21,  0,  0,  0]), array([ 0, 21,  0,  0,  0]), array([ 0, 21,  0,  0,  0]), array([ 0, 21,  0,  0,  0]), array([ 0, 21,  0,  0,  0])]\n","[array([ 0,  0, 19,  0,  2]), array([ 0,  0, 19,  0,  2]), array([ 0,  0, 19,  0,  2]), array([ 0,  0, 19,  0,  2]), array([ 0,  0, 19,  0,  2])]\n","[array([ 0,  1,  0, 20,  0]), array([ 0,  1,  0, 20,  0]), array([ 0,  1,  0, 20,  0]), array([ 0,  1,  0, 20,  0]), array([ 0,  1,  0, 20,  0])]\n","[array([ 0,  1,  1,  0, 19]), array([ 0,  1,  1,  0, 19]), array([ 0,  1,  1,  0, 19]), array([ 0,  1,  1,  0, 19]), array([ 0,  1,  1,  0, 19])]\n"]}]},{"cell_type":"code","source":["PATH = '/content/drive/MyDrive/Tanpopo/model_ViT_weights.pth'\n","#torch.save(model_ViT.state_dict(), PATH) # 重みを保存"],"metadata":{"id":"Rh6swnsVuG-K"},"execution_count":null,"outputs":[]}]}